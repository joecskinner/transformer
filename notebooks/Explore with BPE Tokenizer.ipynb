{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140247a0-9d4d-4d82-b7d8-07c230b34c76",
   "metadata": {},
   "source": [
    "# Explore with BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46424c8-7aaa-4923-a716-ba9d0e09d2bf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c1c6e-50cf-45f0-a150-e11fdee3070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "import src.utils.byte_pair_encoding_tokenizer as bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2862c38-e63f-4eed-8974-656797b72a02",
   "metadata": {},
   "source": [
    "## Initialize BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2add217-3a95-42ea-95d5-b1c1d157858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bpe.CustomBPETokenizer(\n",
    "        [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"], \"../bpe_tokenizers/ted_hrlr_translate_pt_to_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e95b2-13df-40df-abf5-9460d98ae6dc",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65902fc9-cfbc-4f3e-b73a-19cb34e40107",
   "metadata": {},
   "source": [
    "### Step 1: Tokenizing the Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca9920-792b-4509-b270-259564574b90",
   "metadata": {},
   "source": [
    "Given the input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d451c837-b3db-40a3-97bb-30d16523acbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['this is a sentence to be tokenized']\n"
     ]
    }
   ],
   "source": [
    "input = [\"this is a sentence to be tokenized\"]\n",
    "print(f\"Input: {input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfa682-ea18-4741-8412-be618310b5df",
   "metadata": {},
   "source": [
    "The `tokenize` method tokenizes it into a set of integer tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e6e825d-aacd-4e38-8b1d-b8eb3f561920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <tf.RaggedTensor [[2, 693, 186, 120, 7380, 165, 248, 165, 2399, 1609, 3]]>\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.tokenize(input)\n",
    "print(f\"Output: {tokenized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca6291-5e8c-4aeb-bc2a-e1cd0d11ba72",
   "metadata": {},
   "source": [
    "Each integer here corresponds to a token position in the vocabulary. To see the complete generated vocabulary refer to [bpe_tokenizers/ted_hrlr_translate_pt_to_en/vocab.txt](../bpe_tokenizers/ted_hrlr_translate_pt_to_en/vocab.txt). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438268c-9f60-4b3d-a2a3-f9601db075e1",
   "metadata": {},
   "source": [
    "### Step 2: Unveiling Tokens via Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759838d-2a44-4c14-a425-3122f58ac7cf",
   "metadata": {},
   "source": [
    "It is possible to see what characters this integer tokens correspond to using the `lookup` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6582f0a-b66c-4948-a083-58133bd52cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[START]', 'this', 'Ġis', 'Ġa', 'Ġsentence', 'Ġto', 'Ġbe', 'Ġto', 'ken', 'ized', '[END]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.lookup(tokenized)\n",
    "tokens = tokens.to_list()\n",
    "decoded_tokens = [token.decode('utf-8') for sublist in tokens for token in sublist]\n",
    "print(f\"Tokens: {decoded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72b768-07ab-46d0-9e04-ba87fd02fd7d",
   "metadata": {},
   "source": [
    "#### Key Observations:\n",
    "\n",
    "* **Utilization of `Ġ`**: This character signifies the commencement of a new word.\n",
    "* **Single vs. Multi-Token Words**: While frequent words might attain individual tokens, rarer or absent words (like 'tokenized') break into smaller tokens: 'Ġto', 'ken', and 'ized'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ce539-cf5d-48c9-9c14-7db51e0f9f46",
   "metadata": {},
   "source": [
    "### Step 3: Detokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000240c7-f52f-44df-bc0b-1f75bd12c40d",
   "metadata": {},
   "source": [
    "Retrieving the original sentence is attainable with a call to the detokenize method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8db57cb-a79d-4c22-ae31-3724f914312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detokenizes: [b'this is a sentence to be tokenized']\n"
     ]
    }
   ],
   "source": [
    "detokenized = tokenizer.detokenize(tokenized)\n",
    "print(f\"Detokenizes: {detokenized}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
