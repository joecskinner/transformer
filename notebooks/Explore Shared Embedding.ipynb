{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425d1432-ec4e-4f67-aad9-be95520dcb81",
   "metadata": {},
   "source": [
    "# Explore Shared Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cd569-64a7-4c07-94f7-8ab0969f338a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de08ca63-babd-497b-8b43-3290f7f44c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "from src.models.shared_embedding import SharedEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133ba45-61f8-4a5e-bd7c-73021c7ba734",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be7874-7cae-45a6-bfd7-892593958ee9",
   "metadata": {},
   "source": [
    "In order to get a better feel for operations going on inside the `SharedEmbedding` let's first print the dimensions of the shared embeddings input, weights, and outputs using shared_embedding_dimensions for the following parameter values:\n",
    "\n",
    "* `vocab_size=10`\n",
    "* `d_model=4`\n",
    "\n",
    "> Note: The input to the embedding layer will already have gone through the data pipeline process above and so in order to replicate such a dataset we can simply create a array of vectors of a set length.\n",
    "\n",
    "> Note: The embedding weights are subject to learning during the model training process. The weights illustrated below serve as initial values, which, after just a single forward pass through the SharedEmbedding layer, do not hold meaningful representations. Throughout training, these weights will iteratively adjust to encapsulate more useful and semantically relevant information, driven by the minimization of the model's loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b1af1-7851-4b09-a378-7c268bddd54d",
   "metadata": {},
   "source": [
    "## Initialize SharedEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a463ee-d232-4191-8646-1429d793d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_embedding = SharedEmbedding(vocab_size=10, d_model=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007fbf1-4329-44d0-a445-9ab34d72c9e9",
   "metadata": {},
   "source": [
    "## Print input and ouput for a batch containing 1 input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a37f2-3c6e-4e3e-983e-97dbf4da586a",
   "metadata": {},
   "source": [
    "### Input to Shared Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36fc2f01-7620-4d5e-962f-79e4308afe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (batch_size, seq_length): (1, 5)\n",
      "[[3 1 9 8 7]]\n"
     ]
    }
   ],
   "source": [
    "dummy_sentence = np.array([[3, 1, 9, 8, 7]])\n",
    "\n",
    "print(f\"Dimensions (batch_size, seq_length): {dummy_sentence.shape}\")\n",
    "print(f\"{dummy_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee23b93-1129-41b3-9dad-0aa582dbae36",
   "metadata": {},
   "source": [
    "## Output of Shared Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3d8dc43-6f62-495f-84e5-9b3fdee833d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (batch_size, seq_length): (1, 5, 4)\n",
      "[[[-0.09908666  0.06959189 -0.05134678  0.06398902]\n",
      "  [ 0.07289138  0.05813029 -0.00244436 -0.00594692]\n",
      "  [-0.05870557  0.08982063  0.0197406   0.04603086]\n",
      "  [-0.09961352  0.01636253  0.01097564  0.04978543]\n",
      "  [-0.02301206 -0.01816873  0.03561945 -0.0379561 ]]]\n"
     ]
    }
   ],
   "source": [
    "embedded_output = shared_embedding(dummy_sentence)\n",
    "\n",
    "print(f\"Dimensions (batch_size, seq_length): {embedded_output.shape}\")\n",
    "print(f\"{embedded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1a2f9-e727-4d31-82be-4e6156e4c157",
   "metadata": {},
   "source": [
    "## Print input and ouput for a batch containing 3 input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaea9ec-c83c-4531-b98b-6b4b122bf9e5",
   "metadata": {},
   "source": [
    "### Input to Shared Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb0dab3e-869a-4cc8-a99a-d3d969da9b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (batch_size, seq_length): (3, 5)\n",
      "[[3 1 9 8 7]\n",
      " [0 2 3 4 5]\n",
      " [6 7 1 8 9]]\n"
     ]
    }
   ],
   "source": [
    "dummy_sentences = np.array([\n",
    "        [3, 1, 9, 8, 7],\n",
    "        [0, 2, 3, 4, 5],\n",
    "        [6, 7, 1, 8, 9]\n",
    "])\n",
    "\n",
    "print(f\"Dimensions (batch_size, seq_length): {dummy_sentences.shape}\")\n",
    "print(f\"{dummy_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49b719-6684-47ee-bf5b-58b94e0b8e78",
   "metadata": {},
   "source": [
    "### Output of Shared Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "790ec646-3801-48b2-a41f-7ec9de9f7593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (batch_size, seq_length): (3, 5, 4)\n",
      "[[[-0.09908666  0.06959189 -0.05134678  0.06398902]\n",
      "  [ 0.07289138  0.05813029 -0.00244436 -0.00594692]\n",
      "  [-0.05870557  0.08982063  0.0197406   0.04603086]\n",
      "  [-0.09961352  0.01636253  0.01097564  0.04978543]\n",
      "  [-0.02301206 -0.01816873  0.03561945 -0.0379561 ]]\n",
      "\n",
      " [[ 0.04770365 -0.00831404  0.09543822  0.00717072]\n",
      "  [-0.03777781  0.03604748 -0.03230224  0.09779359]\n",
      "  [-0.09908666  0.06959189 -0.05134678  0.06398902]\n",
      "  [-0.04405236 -0.08592837  0.02742467  0.09911964]\n",
      "  [-0.06826501  0.09861846  0.02513096 -0.02877231]]\n",
      "\n",
      " [[ 0.02805883 -0.08670046  0.00149784  0.05752458]\n",
      "  [-0.02301206 -0.01816873  0.03561945 -0.0379561 ]\n",
      "  [ 0.07289138  0.05813029 -0.00244436 -0.00594692]\n",
      "  [-0.09961352  0.01636253  0.01097564  0.04978543]\n",
      "  [-0.05870557  0.08982063  0.0197406   0.04603086]]]\n"
     ]
    }
   ],
   "source": [
    "embedded_output = shared_embedding(dummy_sentences)\n",
    "\n",
    "print(f\"Dimensions (batch_size, seq_length): {embedded_output.shape}\")\n",
    "print(f\"{embedded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f7d86-703d-4e98-a88b-cf6c5b9c2151",
   "metadata": {},
   "source": [
    "## Print the embedding maxtrix weights for the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcbba1f8-2b38-433e-8428-a4e4cd62bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (vocab_size, d_model): (10, 4)\n",
      "[[ 0.04770365 -0.00831404  0.09543822  0.00717072]\n",
      " [ 0.07289138  0.05813029 -0.00244436 -0.00594692]\n",
      " [-0.03777781  0.03604748 -0.03230224  0.09779359]\n",
      " [-0.09908666  0.06959189 -0.05134678  0.06398902]\n",
      " [-0.04405236 -0.08592837  0.02742467  0.09911964]\n",
      " [-0.06826501  0.09861846  0.02513096 -0.02877231]\n",
      " [ 0.02805883 -0.08670046  0.00149784  0.05752458]\n",
      " [-0.02301206 -0.01816873  0.03561945 -0.0379561 ]\n",
      " [-0.09961352  0.01636253  0.01097564  0.04978543]\n",
      " [-0.05870557  0.08982063  0.0197406   0.04603086]]\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = shared_embedding.get_embedding()\n",
    "print(\"Dimensions (vocab_size, d_model): {}\".format(embedding_weights.shape))\n",
    "print(f\"{embedding_weights}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
